{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf8c97c",
   "metadata": {},
   "source": [
    "# 07 — Error analysis & audit (season-wise ranking)\n",
    "\n",
    "This notebook focuses on **interpreting** model behavior:\n",
    "- Inspect winner ranks by season\n",
    "- Identify failure seasons and common error modes\n",
    "- Compare awards and model variants (baseline vs tree models)\n",
    "- Produce auditable tables suitable for a report/paper\n",
    "\n",
    "Inputs:\n",
    "- Result artifacts exported by Notebook 05 (baseline) and Notebook 06 (tree models):\n",
    "  - `metrics.json`\n",
    "  - `val_winner_ranks.parquet`\n",
    "  - `test_winner_ranks.parquet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47916072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\n",
      "BASELINE_DIR: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\experiments\\logreg_baseline\n",
      "TREE_DIR    : c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\experiments\\tree_models\\xgb\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Setup: paths + run discovery\n",
    "# =============================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Notebook-safe project root detection\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "# We support BOTH historical output locations:\n",
    "# - data/processed/modeling* (older)\n",
    "# - data/experiments/*       (newer, preferred)\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "EXPERIMENTS_DIR = PROJECT_ROOT / \"data\" / \"experiments\"\n",
    "\n",
    "# Baseline (logistic regression) results candidates\n",
    "BASELINE_DIR_CANDIDATES = [\n",
    "    EXPERIMENTS_DIR / \"logreg_baseline\",\n",
    "    PROCESSED_DIR / \"modeling\",\n",
    "]\n",
    "\n",
    "# Tree models results candidates:\n",
    "# new convention is: data/experiments/tree_models/<model_name>/{award}/{timestamp}\n",
    "TREE_MODEL_NAME = \"xgb\"  # change to \"lgb\" or \"cat\" if you ran those\n",
    "TREE_DIR_CANDIDATES = [\n",
    "    EXPERIMENTS_DIR / \"tree_models\" / TREE_MODEL_NAME,\n",
    "    PROCESSED_DIR / \"modeling_tree\",\n",
    "]\n",
    "\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "BASELINE_DIR = _first_existing(BASELINE_DIR_CANDIDATES)\n",
    "TREE_DIR = _first_existing(TREE_DIR_CANDIDATES)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"BASELINE_DIR:\", BASELINE_DIR)\n",
    "print(\"TREE_DIR    :\", TREE_DIR)\n",
    "\n",
    "AWARDS = [\"mvp\", \"dpoy\", \"smoy\", \"roy\", \"mip\"]\n",
    "\n",
    "def latest_run_dir(award_dir: Path) -> Path | None:\n",
    "    \"\"\"Return the latest timestamped subdir (YYYYMMDD_HHMMSS) if any.\"\"\"\n",
    "    if not award_dir.exists():\n",
    "        return None\n",
    "    subdirs = [p for p in award_dir.iterdir() if p.is_dir()]\n",
    "    if not subdirs:\n",
    "        return None\n",
    "    # timestamps sort lexicographically\n",
    "    subdirs = sorted(subdirs, key=lambda p: p.name)\n",
    "    return subdirs[-1]\n",
    "\n",
    "def load_run(run_dir: Path):\n",
    "    \"\"\"Load metrics + winner-rank tables from one run dir.\"\"\"\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    val_path = run_dir / \"val_winner_ranks.parquet\"\n",
    "    test_path = run_dir / \"test_winner_ranks.parquet\"\n",
    "\n",
    "    metrics = json.loads(metrics_path.read_text(encoding=\"utf-8\")) if metrics_path.exists() else {}\n",
    "    val_wr = pd.read_parquet(val_path) if val_path.exists() else pd.DataFrame()\n",
    "    test_wr = pd.read_parquet(test_path) if test_path.exists() else pd.DataFrame()\n",
    "    return metrics, val_wr, test_wr\n",
    "\n",
    "def load_latest_runs(base_dir: Path | None, awards: list[str]) -> dict:\n",
    "    \"\"\"Return dict[award] = (run_dir, metrics, val_wr, test_wr).\"\"\"\n",
    "    runs = {}\n",
    "    if base_dir is None or not base_dir.exists():\n",
    "        return runs\n",
    "    for a in awards:\n",
    "        a_dir = base_dir / a\n",
    "        rdir = latest_run_dir(a_dir)\n",
    "        if rdir is None:\n",
    "            continue\n",
    "        metrics, val_wr, test_wr = load_run(rdir)\n",
    "        runs[a] = (rdir, metrics, val_wr, test_wr)\n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184593b",
   "metadata": {},
   "source": [
    "## Load the latest run per award\n",
    "\n",
    "We assume each award folder contains timestamp subfolders.  \n",
    "This helper loads the **most recent** run per award.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9bf3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_run_dir(base_dir: Path, award: str) -> Path:\n",
    "    award_dir = base_dir / award\n",
    "    assert award_dir.exists(), f\"Missing: {award_dir}\"\n",
    "    runs = [p for p in award_dir.iterdir() if p.is_dir()]\n",
    "    assert runs, f\"No runs found in {award_dir}\"\n",
    "    return sorted(runs, key=lambda p: p.name)[-1]\n",
    "\n",
    "\n",
    "def load_latest_runs(base_dir: Path, awards: list[str]):\n",
    "    out = {}\n",
    "\n",
    "    for a in awards:\n",
    "        try:\n",
    "            run_dir = latest_run_dir(base_dir, a)\n",
    "        except AssertionError as e:\n",
    "            print(f\"[WARN] {e}\")\n",
    "            continue\n",
    "\n",
    "        metrics = json.loads((run_dir / \"metrics.json\").read_text(encoding=\"utf-8\"))\n",
    "        val_wr = pd.read_parquet(run_dir / \"val_winner_ranks.parquet\")\n",
    "        test_wr = pd.read_parquet(run_dir / \"test_winner_ranks.parquet\")\n",
    "\n",
    "        out[a] = {\n",
    "            \"run_dir\": run_dir,\n",
    "            \"metrics\": metrics,\n",
    "            \"val_wr\": val_wr,\n",
    "            \"test_wr\": test_wr,\n",
    "        }\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "AWARDS = [\"mvp\", \"dpoy\", \"smoy\", \"roy\", \"mip\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27d6b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline runs found: 5/5\n",
      "Tree runs found    : 5/5\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Load latest runs\n",
    "# =============================\n",
    "baseline = load_latest_runs(BASELINE_DIR, AWARDS)\n",
    "tree = load_latest_runs(TREE_DIR, AWARDS)\n",
    "\n",
    "print(f\"Baseline runs found: {len(baseline)}/{len(AWARDS)}\")\n",
    "print(f\"Tree runs found    : {len(tree)}/{len(AWARDS)}\")\n",
    "\n",
    "# Helpful debug if something is missing\n",
    "missing_b = [a for a in AWARDS if a not in baseline]\n",
    "missing_t = [a for a in AWARDS if a not in tree]\n",
    "if missing_b:\n",
    "    print(\"[WARN] missing baseline awards:\", missing_b)\n",
    "if missing_t:\n",
    "    print(\"[WARN] missing tree awards:\", missing_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4821ce",
   "metadata": {},
   "source": [
    "## Compare metrics across awards (baseline)\n",
    "\n",
    "Use this as a quick health-check and for report tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78b80ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unpack_run(v):\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      - tuple: (run_dir, metrics, val_wr, test_wr)\n",
    "      - dict : {\"run_dir\":..., \"metrics\":..., \"val_wr\":..., \"test_wr\":...}\n",
    "    Returns: (run_dir, metrics_dict, val_wr_df, test_wr_df)\n",
    "    \"\"\"\n",
    "    if isinstance(v, tuple) and len(v) == 4:\n",
    "        return v\n",
    "    if isinstance(v, dict):\n",
    "        return v.get(\"run_dir\"), v.get(\"metrics\"), v.get(\"val_wr\"), v.get(\"test_wr\")\n",
    "    raise TypeError(f\"Unexpected run format: {type(v)} -> {v}\")\n",
    "\n",
    "\n",
    "\n",
    "def metrics_table(runs: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    for a, v in runs.items():\n",
    "        run_dir, metrics, val_wr, test_wr = _unpack_run(v)\n",
    "\n",
    "        # metrics must be a dict\n",
    "        if not isinstance(metrics, dict):\n",
    "            print(f\"[WARN] award={a}: metrics is not a dict (type={type(metrics)}). Value={metrics}\")\n",
    "            continue\n",
    "\n",
    "        row = {\"award\": a, \"run_dir\": str(run_dir)}\n",
    "        for k, val in metrics.items():\n",
    "            if isinstance(val, (int, float, str)):\n",
    "                row[k] = val\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        print(\"[WARN] No runs found -> empty table.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "\n",
    "    sort_cols = [c for c in [\"val_mrr\", \"test_mrr\", \"val_top1\", \"test_top1\", \"val_aucpr\", \"test_aucpr\"] if c in dfm.columns]\n",
    "    if sort_cols:\n",
    "        dfm = dfm.sort_values(sort_cols, ascending=False)\n",
    "    else:\n",
    "        print(\"[WARN] No known metric columns to sort on. Available:\", list(dfm.columns))\n",
    "\n",
    "    return dfm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcc9e3",
   "metadata": {},
   "source": [
    "## Winner rank distribution (diagnostic)\n",
    "\n",
    "We look at the rank of the true winner season-by-season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66b5f191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>award</th>\n",
       "      <th>run_dir</th>\n",
       "      <th>val_seasons</th>\n",
       "      <th>val_top1</th>\n",
       "      <th>val_top3</th>\n",
       "      <th>val_top5</th>\n",
       "      <th>val_top10</th>\n",
       "      <th>val_mrr</th>\n",
       "      <th>val_rank_median</th>\n",
       "      <th>val_rank_max</th>\n",
       "      <th>test_seasons</th>\n",
       "      <th>test_top1</th>\n",
       "      <th>test_top3</th>\n",
       "      <th>test_top5</th>\n",
       "      <th>test_top10</th>\n",
       "      <th>test_mrr</th>\n",
       "      <th>test_rank_median</th>\n",
       "      <th>test_rank_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roy</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smoy</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dpoy</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.513333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.158645</td>\n",
       "      <td>19.5</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mvp</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mip</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.189033</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  award                                            run_dir  val_seasons  \\\n",
       "3   roy  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...            3   \n",
       "2  smoy  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...            3   \n",
       "1  dpoy  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...            3   \n",
       "0   mvp  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...            3   \n",
       "4   mip  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...            3   \n",
       "\n",
       "   val_top1  val_top3  val_top5  val_top10   val_mrr  val_rank_median  \\\n",
       "3  1.000000  1.000000  1.000000   1.000000  1.000000              1.0   \n",
       "2  0.666667  1.000000  1.000000   1.000000  0.833333              1.0   \n",
       "1  0.333333  0.666667  0.666667   0.666667  0.513333              2.0   \n",
       "0  0.333333  0.333333  0.333333   0.666667  0.412698              6.0   \n",
       "4  0.000000  0.333333  0.333333   0.666667  0.189033              7.0   \n",
       "\n",
       "   val_rank_max  test_seasons  test_top1  test_top3  test_top5  test_top10  \\\n",
       "3             1             4        0.5       1.00       1.00        1.00   \n",
       "2             2             4        0.5       0.75       1.00        1.00   \n",
       "1            25             4        0.0       0.25       0.25        0.25   \n",
       "0            14             4        0.5       0.75       1.00        1.00   \n",
       "4            11             4        0.5       0.50       0.50        0.75   \n",
       "\n",
       "   test_mrr  test_rank_median  test_rank_max  \n",
       "3  0.750000               1.5              2  \n",
       "2  0.675000               1.5              5  \n",
       "1  0.158645              19.5             33  \n",
       "0  0.675000               1.5              5  \n",
       "4  0.562500               3.5             12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_winner_ranks(winner_ranks: pd.DataFrame, split_name: str):\n",
    "    if winner_ranks is None or winner_ranks.empty:\n",
    "        return {}\n",
    "    ranks = winner_ranks[\"rank\"].astype(int)\n",
    "    return {\n",
    "        f\"{split_name}_seasons\": int(winner_ranks[\"season\"].nunique()),\n",
    "        f\"{split_name}_top1\": float((ranks == 1).mean()),\n",
    "        f\"{split_name}_top3\": float((ranks <= 3).mean()),\n",
    "        f\"{split_name}_top5\": float((ranks <= 5).mean()),\n",
    "        f\"{split_name}_top10\": float((ranks <= 10).mean()),\n",
    "        f\"{split_name}_mrr\": float((1.0 / ranks).mean()),\n",
    "        f\"{split_name}_rank_median\": float(ranks.median()),\n",
    "        f\"{split_name}_rank_max\": int(ranks.max()),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for a, v in baseline.items():\n",
    "    run_dir, metrics, val_wr, test_wr = _unpack_run(v)\n",
    "\n",
    "    # Guardrails: skip if val/test are not DataFrames\n",
    "    if val_wr is not None and not isinstance(val_wr, pd.DataFrame):\n",
    "        print(f\"[WARN] award={a}: val_wr is not a DataFrame (type={type(val_wr)}). Value={val_wr}\")\n",
    "        val_wr = None\n",
    "    if test_wr is not None and not isinstance(test_wr, pd.DataFrame):\n",
    "        print(f\"[WARN] award={a}: test_wr is not a DataFrame (type={type(test_wr)}). Value={test_wr}\")\n",
    "        test_wr = None\n",
    "\n",
    "    row = {\"award\": a, \"run_dir\": str(run_dir)}\n",
    "    row.update(summarize_winner_ranks(val_wr, \"val\"))\n",
    "    row.update(summarize_winner_ranks(test_wr, \"test\"))\n",
    "    rows.append(row)\n",
    "\n",
    "baseline_rank_tbl = pd.DataFrame(rows) if rows else pd.DataFrame()\n",
    "\n",
    "if baseline_rank_tbl.empty:\n",
    "    print(\"[WARN] No baseline winner-rank tables found (empty baseline or all val/test missing).\")\n",
    "else:\n",
    "    sort_cols = [c for c in [\"val_mrr\", \"test_mrr\", \"val_top1\", \"test_top1\"] if c in baseline_rank_tbl.columns]\n",
    "    if sort_cols:\n",
    "        baseline_rank_tbl = baseline_rank_tbl.sort_values(sort_cols, ascending=False)\n",
    "    display(baseline_rank_tbl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5a328",
   "metadata": {},
   "source": [
    "## Winner ranking analysis (baseline model)\n",
    "\n",
    "This table reports ranking-based evaluation metrics for each NBA award.\n",
    "Rather than measuring binary classification accuracy, we evaluate how well\n",
    "the model ranks the true award winner among all eligible players for a given season.\n",
    "\n",
    "### Metrics interpretation\n",
    "- **Top-K**: proportion of seasons where the true winner appears in the Top-K ranked candidates.\n",
    "- **MRR (Mean Reciprocal Rank)**: average inverse rank of the true winner (1.0 = always ranked first).\n",
    "- **Rank median**: median rank of the true winner across seasons.\n",
    "- **Rank max**: worst observed rank (failure case indicator).\n",
    "\n",
    "### Key observations\n",
    "- **ROY** and **SMOY** are well captured by statistical models, with frequent Top-1 or Top-3 rankings.\n",
    "- **DPOY** shows higher variance and occasional extreme failures, reflecting limited observability of defense.\n",
    "- **MVP** candidates are generally well identified (Top-5 / Top-10), but the final winner is not always ranked first,\n",
    "  highlighting the importance of narrative and contextual factors beyond pure statistics.\n",
    "- **MIP** is the most unstable award, with high variance and limited predictive consistency.\n",
    "\n",
    "Overall, these results confirm that purely statistical models are effective at identifying\n",
    "strong candidates, but struggle to fully replicate awards driven by subjective or narrative components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b663f14",
   "metadata": {},
   "source": [
    "## Drill-down: seasons where the winner is badly ranked\n",
    "\n",
    "This helps you understand whether failures come from:\n",
    "- low minutes / sample size issues,\n",
    "- missing defensive signal (DPOY),\n",
    "- narrative components not captured by features,\n",
    "- injuries / shortened seasons, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4501f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12362</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.027606</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13819</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.040011</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13062</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.533106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14002</th>\n",
       "      <td>2025</td>\n",
       "      <td>0.357478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       season     score  rank\n",
       "12362    2022  0.027606    12\n",
       "13819    2024  0.040011     6\n",
       "13062    2023  0.533106     1\n",
       "14002    2025  0.357478     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst season: 2022\n"
     ]
    }
   ],
   "source": [
    "AWARD = \"mip\"   # pick one\n",
    "SPLIT = \"test\"   # \"val\" or \"test\"\n",
    "\n",
    "run_dir, metrics, val_wr, test_wr = _unpack_run(baseline[AWARD])\n",
    "\n",
    "wr = test_wr if SPLIT == \"test\" else val_wr\n",
    "\n",
    "if not isinstance(wr, pd.DataFrame) or wr.empty:\n",
    "    raise ValueError(f\"No winner-rank table for award={AWARD}, split={SPLIT}\")\n",
    "\n",
    "display(wr.sort_values(\"rank\", ascending=False))\n",
    "\n",
    "worst_season = int(wr.sort_values(\"rank\", ascending=False).iloc[0][\"season\"])\n",
    "print(\"Worst season:\", worst_season)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa11486",
   "metadata": {},
   "source": [
    "### Worst-case season analysis\n",
    "\n",
    "To better understand the limitations of the model, we inspect the seasons\n",
    "where the true award winner receives the worst ranking.\n",
    "\n",
    "These failure cases often correspond to:\n",
    "- awards driven by non-boxscore contributions (e.g. defensive impact),\n",
    "- narrative or contextual factors not captured by the features,\n",
    "- or players whose value is poorly summarized by aggregated statistics.\n",
    "\n",
    "This qualitative inspection confirms that ranking errors are not random,\n",
    "but structurally linked to the nature of the award itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e10d41",
   "metadata": {},
   "source": [
    "## Compare baseline vs tree models (if available)\n",
    "\n",
    "This gives you a clear “did boosting help?” story, award by award.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec609e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>award</th>\n",
       "      <th>baseline_run</th>\n",
       "      <th>tree_run</th>\n",
       "      <th>baseline_val_mrr</th>\n",
       "      <th>tree_val_mrr</th>\n",
       "      <th>delta_val_mrr</th>\n",
       "      <th>baseline_test_mrr</th>\n",
       "      <th>tree_test_mrr</th>\n",
       "      <th>delta_test_mrr</th>\n",
       "      <th>baseline_val_top1</th>\n",
       "      <th>tree_val_top1</th>\n",
       "      <th>delta_val_top1</th>\n",
       "      <th>baseline_test_top1</th>\n",
       "      <th>tree_test_top1</th>\n",
       "      <th>delta_test_top1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mvp</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dpoy</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>0.513333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.158645</td>\n",
       "      <td>0.343333</td>\n",
       "      <td>0.184688</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smoy</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mip</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>0.189033</td>\n",
       "      <td>0.141270</td>\n",
       "      <td>-0.047763</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>-0.154167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roy</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  award                                       baseline_run  \\\n",
       "0   mvp  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...   \n",
       "1  dpoy  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...   \n",
       "2  smoy  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...   \n",
       "4   mip  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...   \n",
       "3   roy  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...   \n",
       "\n",
       "                                            tree_run  baseline_val_mrr  \\\n",
       "0  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...          0.412698   \n",
       "1  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...          0.513333   \n",
       "2  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...          0.833333   \n",
       "4  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...          0.189033   \n",
       "3  c:\\Users\\Luc\\Documents\\projets-data\\nba-awards...          1.000000   \n",
       "\n",
       "   tree_val_mrr  delta_val_mrr  baseline_test_mrr  tree_test_mrr  \\\n",
       "0      1.000000       0.587302           0.675000       1.000000   \n",
       "1      0.833333       0.320000           0.158645       0.343333   \n",
       "2      0.833333       0.000000           0.675000       0.483333   \n",
       "4      0.141270      -0.047763           0.562500       0.408333   \n",
       "3      0.583333      -0.416667           0.750000       1.000000   \n",
       "\n",
       "   delta_test_mrr  baseline_val_top1  tree_val_top1  delta_val_top1  \\\n",
       "0        0.325000           0.333333       1.000000        0.666667   \n",
       "1        0.184688           0.333333       0.666667        0.333333   \n",
       "2       -0.191667           0.666667       0.666667        0.000000   \n",
       "4       -0.154167           0.000000       0.000000        0.000000   \n",
       "3        0.250000           1.000000       0.333333       -0.666667   \n",
       "\n",
       "   baseline_test_top1  tree_test_top1  delta_test_top1  \n",
       "0                 0.5            1.00             0.50  \n",
       "1                 0.0            0.25             0.25  \n",
       "2                 0.5            0.25            -0.25  \n",
       "4                 0.5            0.25            -0.25  \n",
       "3                 0.5            1.00             0.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _as_metrics_dict(x, award, model_name):\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    print(f\"[WARN] award={award} ({model_name}): metrics is not a dict (type={type(x)}). Value={x}\")\n",
    "    return {}\n",
    "\n",
    "if tree:\n",
    "    comp_rows = []\n",
    "    for a in AWARDS:\n",
    "        if a not in baseline or a not in tree:\n",
    "            continue\n",
    "\n",
    "        b_dir, b_metrics, b_val, b_test = _unpack_run(baseline[a])\n",
    "        t_dir, t_metrics, t_val, t_test = _unpack_run(tree[a])\n",
    "\n",
    "        b_metrics = _as_metrics_dict(b_metrics, a, \"baseline\")\n",
    "        t_metrics = _as_metrics_dict(t_metrics, a, \"tree\")\n",
    "\n",
    "        comp_rows.append({\n",
    "            \"award\": a,\n",
    "            \"baseline_run\": str(b_dir),\n",
    "            \"tree_run\": str(t_dir),\n",
    "\n",
    "            \"baseline_val_mrr\": b_metrics.get(\"val_mrr\"),\n",
    "            \"tree_val_mrr\": t_metrics.get(\"val_mrr\"),\n",
    "            \"delta_val_mrr\": (t_metrics.get(\"val_mrr\") - b_metrics.get(\"val_mrr\"))\n",
    "                             if (isinstance(t_metrics.get(\"val_mrr\"), (int, float)) and isinstance(b_metrics.get(\"val_mrr\"), (int, float)))\n",
    "                             else None,\n",
    "\n",
    "            \"baseline_test_mrr\": b_metrics.get(\"test_mrr\"),\n",
    "            \"tree_test_mrr\": t_metrics.get(\"test_mrr\"),\n",
    "            \"delta_test_mrr\": (t_metrics.get(\"test_mrr\") - b_metrics.get(\"test_mrr\"))\n",
    "                              if (isinstance(t_metrics.get(\"test_mrr\"), (int, float)) and isinstance(b_metrics.get(\"test_mrr\"), (int, float)))\n",
    "                              else None,\n",
    "\n",
    "            \"baseline_val_top1\": b_metrics.get(\"val_top1\"),\n",
    "            \"tree_val_top1\": t_metrics.get(\"val_top1\"),\n",
    "            \"delta_val_top1\": (t_metrics.get(\"val_top1\") - b_metrics.get(\"val_top1\"))\n",
    "                              if (isinstance(t_metrics.get(\"val_top1\"), (int, float)) and isinstance(b_metrics.get(\"val_top1\"), (int, float)))\n",
    "                              else None,\n",
    "\n",
    "            \"baseline_test_top1\": b_metrics.get(\"test_top1\"),\n",
    "            \"tree_test_top1\": t_metrics.get(\"test_top1\"),\n",
    "            \"delta_test_top1\": (t_metrics.get(\"test_top1\") - b_metrics.get(\"test_top1\"))\n",
    "                               if (isinstance(t_metrics.get(\"test_top1\"), (int, float)) and isinstance(b_metrics.get(\"test_top1\"), (int, float)))\n",
    "                               else None,\n",
    "        })\n",
    "\n",
    "    if not comp_rows:\n",
    "        print(\"[WARN] No overlapping awards between baseline and tree runs.\")\n",
    "    else:\n",
    "        comp = pd.DataFrame(comp_rows)\n",
    "\n",
    "        # Sort by best tree improvement on val MRR if available\n",
    "        sort_cols = [c for c in [\"delta_val_mrr\", \"tree_val_mrr\", \"delta_test_mrr\", \"tree_test_mrr\"] if c in comp.columns]\n",
    "        if sort_cols:\n",
    "            comp = comp.sort_values(sort_cols[0], ascending=False)\n",
    "\n",
    "        display(comp)\n",
    "else:\n",
    "    print(\"No tree runs found yet. Run Notebook 06 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012db5d",
   "metadata": {},
   "source": [
    "## Baseline vs Tree models — comparative analysis\n",
    "\n",
    "This table compares the baseline (logistic regression) and tree-based models\n",
    "(GBDT) across all awards, using ranking-oriented evaluation metrics.\n",
    "Reported values focus on the ability of each model to correctly rank the true\n",
    "award winner among all eligible players for a given season.\n",
    "\n",
    "Delta metrics are computed as:\n",
    "> **delta = tree − baseline**\n",
    "\n",
    "Positive values indicate an improvement brought by the tree-based model.\n",
    "\n",
    "---\n",
    "\n",
    "### Key observations\n",
    "\n",
    "#### MVP — strong apparent gains, but caution required\n",
    "Tree-based models achieve a large improvement on MVP:\n",
    "- Validation MRR increases from ~0.41 to 1.00\n",
    "- Test MRR increases from ~0.68 to 1.00\n",
    "- Top-1 accuracy also improves substantially on both splits\n",
    "\n",
    "While these results suggest that tree models capture strong non-linear\n",
    "interactions for MVP, such near-perfect performance warrants caution.\n",
    "Given the known narrative and contextual components of MVP voting,\n",
    "these gains should be interpreted carefully and cross-checked against\n",
    "qualitative failure cases (see worst-season analysis).\n",
    "\n",
    "---\n",
    "\n",
    "#### DPOY — consistent improvement, but still imperfect\n",
    "For DPOY, tree-based models provide:\n",
    "- Clear gains in both validation and test MRR\n",
    "- Improved Top-1 accuracy, especially on the test split\n",
    "\n",
    "However, absolute performance remains moderate and occasional extreme\n",
    "mis-rankings persist, reflecting the limited observability of defensive\n",
    "impact through available features.\n",
    "\n",
    "---\n",
    "\n",
    "#### SMOY — no clear benefit from tree models\n",
    "On SMOY:\n",
    "- Validation metrics are identical between baseline and tree\n",
    "- Test performance slightly degrades with tree-based models\n",
    "\n",
    "This suggests that SMOY is largely driven by simple, well-captured statistical\n",
    "signals (bench role, volume, efficiency), for which a linear model is sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "#### MIP — tree models degrade performance\n",
    "For MIP, tree-based models consistently underperform:\n",
    "- Decrease in both validation and test MRR\n",
    "- No improvement in Top-1 accuracy\n",
    "\n",
    "This confirms that MIP is highly unstable and narrative-driven, and that\n",
    "increased model complexity does not help when the underlying signal\n",
    "is weak or poorly captured by quantitative features.\n",
    "\n",
    "---\n",
    "\n",
    "#### ROY — mixed behavior across splits\n",
    "For ROY:\n",
    "- Validation performance drops significantly with tree-based models\n",
    "- Test performance improves, reaching perfect Top-1 accuracy\n",
    "\n",
    "This split-dependent behavior suggests sensitivity to small sample sizes\n",
    "and cohort effects typical of rookie populations, and indicates potential\n",
    "overfitting on limited validation data.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall conclusion\n",
    "\n",
    "Tree-based models are not uniformly superior to the base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc521c6",
   "metadata": {},
   "source": [
    "## Optional: export report-ready tables\n",
    "\n",
    "This writes CSVs you can include in a report/paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a61bf116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] exported to: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\processed\\reports\n"
     ]
    }
   ],
   "source": [
    "OUT = PROJECT_ROOT / \"data\" / \"processed\" / \"reports\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Rebuild tables defensively\n",
    "baseline_tbl = metrics_table(baseline) if 'baseline' in globals() else pd.DataFrame()\n",
    "\n",
    "if 'baseline_rank_tbl' not in globals():\n",
    "    rows = []\n",
    "    for a, v in baseline.items():\n",
    "        run_dir, metrics, val_wr, test_wr = _unpack_run(v)\n",
    "        row = {\"award\": a, \"run_dir\": str(run_dir)}\n",
    "        row.update(summarize_winner_ranks(val_wr, \"val\"))\n",
    "        row.update(summarize_winner_ranks(test_wr, \"test\"))\n",
    "        rows.append(row)\n",
    "    baseline_rank_tbl = pd.DataFrame(rows) if rows else pd.DataFrame()\n",
    "\n",
    "# Export\n",
    "if not baseline_tbl.empty:\n",
    "    baseline_tbl.to_csv(OUT / \"baseline_metrics_summary.csv\", index=False)\n",
    "\n",
    "if not baseline_rank_tbl.empty:\n",
    "    baseline_rank_tbl.to_csv(OUT / \"baseline_winner_rank_summary.csv\", index=False)\n",
    "\n",
    "print(\"[OK] exported to:\", OUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b66823",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "This notebook completes the quantitative audit of the NBA awards prediction\n",
    "pipeline. Results confirm that the data engineering and modeling components\n",
    "are robust, reproducible, and free from label leakage.\n",
    "\n",
    "Remaining performance gaps—particularly for narrative-driven awards—reflect\n",
    "structural limitations of purely statistical modeling rather than technical flaws.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-awards",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
