{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7154c661",
   "metadata": {},
   "source": [
    "# Building Model-Ready Feature Matrices (`X_df`)\n",
    "\n",
    "This notebook transforms `df_clean` into **numerical feature matrices**\n",
    "suitable for machine learning models.\n",
    "\n",
    "Two feature matrices are constructed to handle differences in metric\n",
    "availability across NBA eras:\n",
    "\n",
    "- **Era-wide matrix (1996–2024)**  \n",
    "  Uses only internally computed, season-relative percentiles to ensure\n",
    "  full historical coverage and temporal consistency.\n",
    "\n",
    "- **Modern-era matrix (≥2014)**  \n",
    "  Extends the base feature set with external advanced metrics\n",
    "  (RAPTOR, LEBRON, MAMBA) when available.\n",
    "\n",
    "All operations are purely feature-oriented:\n",
    "no award-specific filtering, labeling, or modeling assumptions\n",
    "are introduced at this stage.\n",
    "\n",
    "**Outputs:**\n",
    "- `X_df_era.parquet`\n",
    "- `X_df_modern.parquet`\n",
    "- CSV files documenting the selected feature lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6e0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\n",
      "DATA_PROCESSED: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Project root (robust, no pyproject.toml needed)\n",
    "# Looks for common \"project markers\": .git, data/, notebooks/, README.md\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "MARKERS = [\n",
    "    \".git\",\n",
    "    \"data\",\n",
    "    \"notebooks\",\n",
    "    \"README.md\",\n",
    "]\n",
    "\n",
    "def is_project_root(p: Path) -> bool:\n",
    "    return any((p / m).exists() for m in MARKERS)\n",
    "\n",
    "while not is_project_root(PROJECT_ROOT):\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\n",
    "            \"Project root not found. Run this notebook from inside the repo \"\n",
    "            \"(a folder containing one of: .git, data/, notebooks/, README.md).\"\n",
    "        )\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "DATA_PROCESSED_FINAL = DATA_PROCESSED / \"players\" / \"final\"\n",
    "DATA_INTERIM = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "DATA_INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "162cda91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_clean shape: (13842, 427)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Load df_clean produced by notebook 02\n",
    "# -----------------------------\n",
    "DF_CLEAN_PATH = DATA_INTERIM / \"df_clean.parquet\"\n",
    "df_clean = pd.read_parquet(DF_CLEAN_PATH)\n",
    "print(\"df_clean shape:\", df_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2642c3",
   "metadata": {},
   "source": [
    "## 1) Define feature families\n",
    "\n",
    "We use **percentiles** as the primary modeling representation because:\n",
    "- they are season-normalized,\n",
    "- reduce era drift,\n",
    "- keep feature scale consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d339118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base pct features: 144\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cols_starting_with(df, prefix: str):\n",
    "    return [c for c in df.columns if c.startswith(prefix)]\n",
    "\n",
    "# Base percentiles (from Basketball-Reference derived tables)\n",
    "BASE_PCT_FEATURES = cols_starting_with(df_clean, \"pct_\")\n",
    "\n",
    "# Remove outcome-derived percentiles\n",
    "BASE_PCT_FEATURES = [c for c in BASE_PCT_FEATURES if not re.match(r\"^pct_is_.*winner$\", c)]\n",
    "print(\"Base pct features:\", len(BASE_PCT_FEATURES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66871b49",
   "metadata": {},
   "source": [
    "## 2) External metric percentiles (modern coverage)\n",
    "\n",
    "We only add **percentiles** for external metrics (not raw values nor ranks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "719dade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External pct features (existing): 0\n",
      "External pct features list: []\n"
     ]
    }
   ],
   "source": [
    "EXTERNAL_PCT_COLS = [\n",
    "    # RAPTOR\n",
    "    \"pct_raptor__raptor_total\",\n",
    "    \"pct_raptor__raptor_offense\",\n",
    "    \"pct_raptor__raptor_defense\",\n",
    "    \"pct_raptor__war_total\",\n",
    "    # LEBRON\n",
    "    \"pct_lebron__LEBRON\",\n",
    "    \"pct_lebron__O-LEBRON\",\n",
    "    \"pct_lebron__D-LEBRON\",\n",
    "    # MAMBA\n",
    "    \"pct_mamba__MAMBA\",\n",
    "    \"pct_mamba__O-MAMBA\",\n",
    "    \"pct_mamba__D-MAMBA\",\n",
    "]\n",
    "EXTERNAL_PCT_COLS = [c for c in EXTERNAL_PCT_COLS if c in df_clean.columns]\n",
    "print(\"External pct features (existing):\", len(EXTERNAL_PCT_COLS))\n",
    "print(\"External pct features list:\", EXTERNAL_PCT_COLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd19d99",
   "metadata": {},
   "source": [
    "## 3) Build do-not-use-in-X list (leakage + identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6453744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns excluded from X (any reason): 54\n"
     ]
    }
   ],
   "source": [
    "LEAK_PATTERNS = [\n",
    "    r\".*_rank$\",\n",
    "    r\"^is_.*\",\n",
    "    r\"^all_nba_.*\", r\"^all_def_.*\", r\"^all_rookie_.*\",\n",
    "    r\"^has_.*consideration$\",\n",
    "    r\"^pct_is_.*winner$\",\n",
    "    r\"^pct_is_.*_winner$\",\n",
    "]\n",
    "def is_leak_col(c: str) -> bool:\n",
    "    return any(re.match(p, c) for p in LEAK_PATTERNS)\n",
    "\n",
    "IDENTIFIER_COLS = {\n",
    "    \"Player\", \"player_key\", \"player_name_raw\", \"PLAYER_ID\", \"PLAYER_NAME\",\n",
    "    \"raptor__nba_id\", \"raptor__player_id\", \"raptor__player_name\",\n",
    "    \"lebron__nba_id\", \"lebron__player_name\", \"lebron__Season\",\n",
    "    \"mamba__nba_id\", \"mamba__player_name\",\n",
    "}\n",
    "NON_TABULAR_COLS = {\"teams_list\", \"minutes_list\"}\n",
    "EXCLUDED_CATEGORICAL = {\"Team\"}   # we keep it in df_clean, but don't model it (for now)\n",
    "\n",
    "DROP_FROM_X = {\n",
    "    c for c in df_clean.columns\n",
    "    if is_leak_col(c) or c in IDENTIFIER_COLS or c in NON_TABULAR_COLS or c in EXCLUDED_CATEGORICAL\n",
    "}\n",
    "print(\"Columns excluded from X (any reason):\", len(DROP_FROM_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee32800",
   "metadata": {},
   "source": [
    "## 4) Construct X_df_era and X_df_modern\n",
    "\n",
    "- `X_df_era`: base percentiles + `Pos` (categorical)\n",
    "- `X_df_modern`: same, but the external metric percentiles will be **used only for modern-era samples** (≥2014) during modeling.\n",
    "\n",
    "We keep *one* table per variant to make the modeling notebook simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea3670d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_df_era shape: (13842, 145)\n",
      "X_df_modern shape: (13842, 145)\n"
     ]
    }
   ],
   "source": [
    "# Keep only the columns we intend to use (+ Pos for one-hot)\n",
    "CORE_COLS = [\"Pos\"] if \"Pos\" in df_clean.columns else []\n",
    "X_base = df_clean[CORE_COLS + BASE_PCT_FEATURES].copy()\n",
    "\n",
    "# Modern X has extra columns (even if many NaN pre-2014)\n",
    "X_modern = df_clean[CORE_COLS + BASE_PCT_FEATURES + EXTERNAL_PCT_COLS].copy()\n",
    "\n",
    "X_df_era = X_base\n",
    "X_df_modern = X_modern\n",
    "\n",
    "print(\"X_df_era shape:\", X_df_era.shape)\n",
    "print(\"X_df_modern shape:\", X_df_modern.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd55c23",
   "metadata": {},
   "source": [
    "## 5) Quick audit (types, missingness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45009b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== X_df_era =="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Object cols: 1\n",
      "Array-like object cols (should be 0): []\n",
      "All-NaN numeric cols: 0\n",
      "== X_df_modern ==\n",
      "Object cols: 1\n",
      "Array-like object cols (should be 0): []\n",
      "All-NaN numeric cols: 0\n"
     ]
    }
   ],
   "source": [
    "def audit_X(X: pd.DataFrame, name: str):\n",
    "    obj_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    array_like = []\n",
    "    for c in obj_cols:\n",
    "        sample = X[c].dropna().head(50)\n",
    "        if any(isinstance(v, (list, tuple, dict, set, np.ndarray)) for v in sample):\n",
    "            array_like.append(c)\n",
    "\n",
    "    numeric_cols = [c for c in X.columns if c not in obj_cols]\n",
    "    all_nan_numeric = [c for c in numeric_cols if X[c].isna().all()]\n",
    "\n",
    "    print(f\"== {name} ==\")\n",
    "    print(\"Object cols:\", len(obj_cols))\n",
    "    print(\"Array-like object cols (should be 0):\", array_like)\n",
    "    print(\"All-NaN numeric cols:\", len(all_nan_numeric))\n",
    "    if all_nan_numeric:\n",
    "        print(\"Example:\", all_nan_numeric[:10])\n",
    "\n",
    "audit_X(X_df_era, \"X_df_era\")\n",
    "audit_X(X_df_modern, \"X_df_modern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae89142",
   "metadata": {},
   "source": [
    "## 6) Save feature matrices and column lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f62b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\interim\\X_df_era.parquet\n",
      "Saved: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\interim\\X_df_modern.parquet\n",
      "Saved: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\interim\\features_era_pct_only.csv\n",
      "Saved: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\interim\\features_modern_pct_plus_externals.csv\n"
     ]
    }
   ],
   "source": [
    "X_ERA_PATH = DATA_INTERIM / \"X_df_era.parquet\"\n",
    "X_MOD_PATH = DATA_INTERIM / \"X_df_modern.parquet\"\n",
    "ERA_COLS_PATH = DATA_INTERIM / \"features_era_pct_only.csv\"\n",
    "MOD_COLS_PATH = DATA_INTERIM / \"features_modern_pct_plus_externals.csv\"\n",
    "\n",
    "X_df_era.to_parquet(X_ERA_PATH, index=False)\n",
    "X_df_modern.to_parquet(X_MOD_PATH, index=False)\n",
    "\n",
    "pd.Series(X_df_era.columns, name=\"feature\").to_csv(ERA_COLS_PATH, index=False)\n",
    "pd.Series(X_df_modern.columns, name=\"feature\").to_csv(MOD_COLS_PATH, index=False)\n",
    "\n",
    "print(\"Saved:\", X_ERA_PATH)\n",
    "print(\"Saved:\", X_MOD_PATH)\n",
    "print(\"Saved:\", ERA_COLS_PATH)\n",
    "print(\"Saved:\", MOD_COLS_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-awards",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
