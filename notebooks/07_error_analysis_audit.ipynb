{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf8c97c",
   "metadata": {},
   "source": [
    "# 07 — Error analysis & audit (season-wise ranking)\n",
    "\n",
    "This notebook focuses on **interpreting** model behavior:\n",
    "- Inspect winner ranks by season\n",
    "- Identify failure seasons and common error modes\n",
    "- Compare awards and model variants (baseline vs tree models)\n",
    "- Produce auditable tables suitable for a report/paper\n",
    "\n",
    "Inputs:\n",
    "- Result artifacts exported by Notebook 05 (baseline) and Notebook 06 (tree models):\n",
    "  - `metrics.json`\n",
    "  - `val_winner_ranks.parquet`\n",
    "  - `test_winner_ranks.parquet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47916072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\n",
      "BASELINE_DIR: c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\experiments\\logreg_baseline\n",
      "TREE_DIR    : c:\\Users\\Luc\\Documents\\projets-data\\nba-awards-predictor\\data\\experiments\\tree_models\\xgb\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Setup: paths + run discovery\n",
    "# =============================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Notebook-safe project root detection\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "# We support BOTH historical output locations:\n",
    "# - data/processed/modeling* (older)\n",
    "# - data/experiments/*       (newer, preferred)\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "EXPERIMENTS_DIR = PROJECT_ROOT / \"data\" / \"experiments\"\n",
    "\n",
    "# Baseline (logistic regression) results candidates\n",
    "BASELINE_DIR_CANDIDATES = [\n",
    "    EXPERIMENTS_DIR / \"logreg_baseline\",\n",
    "    PROCESSED_DIR / \"modeling\",\n",
    "]\n",
    "\n",
    "# Tree models results candidates:\n",
    "# new convention is: data/experiments/tree_models/<model_name>/{award}/{timestamp}\n",
    "TREE_MODEL_NAME = \"xgb\"  # change to \"lgb\" or \"cat\" if you ran those\n",
    "TREE_DIR_CANDIDATES = [\n",
    "    EXPERIMENTS_DIR / \"tree_models\" / TREE_MODEL_NAME,\n",
    "    PROCESSED_DIR / \"modeling_tree\",\n",
    "]\n",
    "\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "BASELINE_DIR = _first_existing(BASELINE_DIR_CANDIDATES)\n",
    "TREE_DIR = _first_existing(TREE_DIR_CANDIDATES)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"BASELINE_DIR:\", BASELINE_DIR)\n",
    "print(\"TREE_DIR    :\", TREE_DIR)\n",
    "\n",
    "AWARDS = [\"mvp\", \"dpoy\", \"smoy\", \"roy\", \"mip\"]\n",
    "\n",
    "def latest_run_dir(award_dir: Path) -> Path | None:\n",
    "    \"\"\"Return the latest timestamped subdir (YYYYMMDD_HHMMSS) if any.\"\"\"\n",
    "    if not award_dir.exists():\n",
    "        return None\n",
    "    subdirs = [p for p in award_dir.iterdir() if p.is_dir()]\n",
    "    if not subdirs:\n",
    "        return None\n",
    "    # timestamps sort lexicographically\n",
    "    subdirs = sorted(subdirs, key=lambda p: p.name)\n",
    "    return subdirs[-1]\n",
    "\n",
    "def load_run(run_dir: Path):\n",
    "    \"\"\"Load metrics + winner-rank tables from one run dir.\"\"\"\n",
    "    metrics_path = run_dir / \"metrics.json\"\n",
    "    val_path = run_dir / \"val_winner_ranks.parquet\"\n",
    "    test_path = run_dir / \"test_winner_ranks.parquet\"\n",
    "\n",
    "    metrics = json.loads(metrics_path.read_text(encoding=\"utf-8\")) if metrics_path.exists() else {}\n",
    "    val_wr = pd.read_parquet(val_path) if val_path.exists() else pd.DataFrame()\n",
    "    test_wr = pd.read_parquet(test_path) if test_path.exists() else pd.DataFrame()\n",
    "    return metrics, val_wr, test_wr\n",
    "\n",
    "def load_latest_runs(base_dir: Path | None, awards: list[str]) -> dict:\n",
    "    \"\"\"Return dict[award] = (run_dir, metrics, val_wr, test_wr).\"\"\"\n",
    "    runs = {}\n",
    "    if base_dir is None or not base_dir.exists():\n",
    "        return runs\n",
    "    for a in awards:\n",
    "        a_dir = base_dir / a\n",
    "        rdir = latest_run_dir(a_dir)\n",
    "        if rdir is None:\n",
    "            continue\n",
    "        metrics, val_wr, test_wr = load_run(rdir)\n",
    "        runs[a] = (rdir, metrics, val_wr, test_wr)\n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184593b",
   "metadata": {},
   "source": [
    "## Load the latest run per award\n",
    "\n",
    "We assume each award folder contains timestamp subfolders.  \n",
    "This helper loads the **most recent** run per award.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bf3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_run_dir(base_dir: Path, award: str) -> Path:\n",
    "    award_dir = base_dir / award\n",
    "    assert award_dir.exists(), f\"Missing: {award_dir}\"\n",
    "    runs = [p for p in award_dir.iterdir() if p.is_dir()]\n",
    "    assert runs, f\"No runs found in {award_dir}\"\n",
    "    return sorted(runs, key=lambda p: p.name)[-1]\n",
    "\n",
    "\n",
    "def load_run(base_dir: Path, award: str):\n",
    "    run_dir = latest_run_dir(base_dir, award)\n",
    "    metrics = json.loads((run_dir / \"metrics.json\").read_text(encoding=\"utf-8\"))\n",
    "    val_wr = pd.read_parquet(run_dir / \"val_winner_ranks.parquet\")\n",
    "    test_wr = pd.read_parquet(run_dir / \"test_winner_ranks.parquet\")\n",
    "    return run_dir, metrics, val_wr, test_wr\n",
    "\n",
    "\n",
    "AWARDS = [\"mvp\", \"dpoy\", \"smoy\", \"roy\", \"mip\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d6b0e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "latest_run_dir() missing 1 required positional argument: 'award'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Load latest runs\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m baseline = \u001b[43mload_latest_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASELINE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAWARDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m tree = load_latest_runs(TREE_DIR, AWARDS)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBaseline runs found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(baseline)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(AWARDS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mload_latest_runs\u001b[39m\u001b[34m(base_dir, awards)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m awards:\n\u001b[32m     76\u001b[39m     a_dir = base_dir / a\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     rdir = \u001b[43mlatest_run_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     79\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: latest_run_dir() missing 1 required positional argument: 'award'"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Load latest runs\n",
    "# =============================\n",
    "baseline = load_latest_runs(BASELINE_DIR, AWARDS)\n",
    "tree = load_latest_runs(TREE_DIR, AWARDS)\n",
    "\n",
    "print(f\"Baseline runs found: {len(baseline)}/{len(AWARDS)}\")\n",
    "print(f\"Tree runs found    : {len(tree)}/{len(AWARDS)}\")\n",
    "\n",
    "# Helpful debug if something is missing\n",
    "missing_b = [a for a in AWARDS if a not in baseline]\n",
    "missing_t = [a for a in AWARDS if a not in tree]\n",
    "if missing_b:\n",
    "    print(\"[WARN] missing baseline awards:\", missing_b)\n",
    "if missing_t:\n",
    "    print(\"[WARN] missing tree awards:\", missing_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4821ce",
   "metadata": {},
   "source": [
    "## Compare metrics across awards (baseline)\n",
    "\n",
    "Use this as a quick health-check and for report tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b80ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_table(runs: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for a, (run_dir, metrics, val_wr, test_wr) in runs.items():\n",
    "        row = {\"award\": a, \"run_dir\": str(run_dir)}\n",
    "        for k, v in metrics.items():\n",
    "            if isinstance(v, (int, float, str)):\n",
    "                row[k] = v\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        print(\"[WARN] No runs found -> empty table.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dfm = pd.DataFrame(rows)\n",
    "\n",
    "    # Robust sort: use the best available columns\n",
    "    sort_cols = [c for c in [\"val_mrr\", \"test_mrr\", \"val_top1\", \"test_top1\", \"val_aucpr\", \"test_aucpr\"] if c in dfm.columns]\n",
    "    if sort_cols:\n",
    "        dfm = dfm.sort_values(sort_cols, ascending=False)\n",
    "    else:\n",
    "        print(\"[WARN] No known metric columns to sort on. Available:\", list(dfm.columns))\n",
    "    return dfm\n",
    "\n",
    "baseline_tbl = metrics_table(baseline)\n",
    "display(baseline_tbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcc9e3",
   "metadata": {},
   "source": [
    "## Winner rank distribution (diagnostic)\n",
    "\n",
    "We look at the rank of the true winner season-by-season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_winner_ranks(winner_ranks: pd.DataFrame, split_name: str):\n",
    "    if winner_ranks is None or winner_ranks.empty:\n",
    "        return {}\n",
    "    ranks = winner_ranks[\"rank\"].astype(int)\n",
    "    return {\n",
    "        f\"{split_name}_seasons\": int(winner_ranks[\"season\"].nunique()),\n",
    "        f\"{split_name}_top1\": float((ranks == 1).mean()),\n",
    "        f\"{split_name}_top3\": float((ranks <= 3).mean()),\n",
    "        f\"{split_name}_top5\": float((ranks <= 5).mean()),\n",
    "        f\"{split_name}_top10\": float((ranks <= 10).mean()),\n",
    "        f\"{split_name}_mrr\": float((1.0 / ranks).mean()),\n",
    "        f\"{split_name}_rank_median\": float(ranks.median()),\n",
    "        f\"{split_name}_rank_max\": int(ranks.max()),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for a, (run_dir, metrics, val_wr, test_wr) in baseline.items():\n",
    "    row = {\"award\": a, \"run_dir\": str(run_dir)}\n",
    "    row.update(summarize_winner_ranks(val_wr, \"val\"))\n",
    "    row.update(summarize_winner_ranks(test_wr, \"test\"))\n",
    "    rows.append(row)\n",
    "\n",
    "if not rows:\n",
    "    print(\"[WARN] No baseline winner-rank tables found (baseline dict empty or missing parquet files).\" )\n",
    "    baseline_rank_tbl = pd.DataFrame()\n",
    "else:\n",
    "    baseline_rank_tbl = pd.DataFrame(rows)\n",
    "    sort_cols = [c for c in [\"val_mrr\", \"test_mrr\", \"val_top1\", \"test_top1\"] if c in baseline_rank_tbl.columns]\n",
    "    if sort_cols:\n",
    "        baseline_rank_tbl = baseline_rank_tbl.sort_values(sort_cols, ascending=False)\n",
    "    display(baseline_rank_tbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b663f14",
   "metadata": {},
   "source": [
    "## Drill-down: seasons where the winner is badly ranked\n",
    "\n",
    "This helps you understand whether failures come from:\n",
    "- low minutes / sample size issues,\n",
    "- missing defensive signal (DPOY),\n",
    "- narrative components not captured by features,\n",
    "- injuries / shortened seasons, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4501f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWARD = \"dpoy\"  # pick one\n",
    "SPLIT = \"test\"    # \"val\" or \"test\"\n",
    "\n",
    "run_dir, metrics, val_wr, test_wr = baseline[AWARD]\n",
    "wr = test_wr if SPLIT == \"test\" else val_wr\n",
    "\n",
    "display(wr.sort_values(\"rank\", ascending=False))\n",
    "print(\"Worst season:\", int(wr.sort_values('rank', ascending=False).iloc[0]['season']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e10d41",
   "metadata": {},
   "source": [
    "## Compare baseline vs tree models (if available)\n",
    "\n",
    "This gives you a clear “did boosting help?” story, award by award.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec609e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tree:\n",
    "    comp_rows = []\n",
    "    for a in AWARDS:\n",
    "        if a not in baseline or a not in tree:\n",
    "            continue\n",
    "        b_dir, b_metrics, b_val, b_test = baseline[a]\n",
    "        t_dir, t_metrics, t_val, t_test = tree[a]\n",
    "        comp_rows.append({\n",
    "            \"award\": a,\n",
    "            \"baseline_run\": str(b_dir),\n",
    "            \"tree_run\": str(t_dir),\n",
    "            \"baseline_val_mrr\": b_metrics.get(\"val_mrr\"),\n",
    "            \"tree_val_mrr\": t_metrics.get(\"val_mrr\"),\n",
    "            \"baseline_test_mrr\": b_metrics.get(\"test_mrr\"),\n",
    "            \"tree_test_mrr\": t_metrics.get(\"test_mrr\"),\n",
    "            \"baseline_val_top1\": b_metrics.get(\"val_top1\"),\n",
    "            \"tree_val_top1\": t_metrics.get(\"val_top1\"),\n",
    "            \"baseline_test_top1\": b_metrics.get(\"test_top1\"),\n",
    "            \"tree_test_top1\": t_metrics.get(\"test_top1\"),\n",
    "        })\n",
    "    if not comp_rows:\n",
    "        print(\"[WARN] No overlapping awards between baseline and tree runs.\")\n",
    "    else:\n",
    "        comp = pd.DataFrame(comp_rows)\n",
    "        sort_col = \"tree_val_mrr\" if \"tree_val_mrr\" in comp.columns else None\n",
    "        if sort_col:\n",
    "            comp = comp.sort_values(sort_col, ascending=False)\n",
    "        display(comp)\n",
    "else:\n",
    "    print(\"No tree runs found yet. Run Notebook 06 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc521c6",
   "metadata": {},
   "source": [
    "## Optional: export report-ready tables\n",
    "\n",
    "This writes CSVs you can include in a report/paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = PROJECT_ROOT / \"data\" / \"processed\" / \"reports\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not baseline_tbl.empty:\n",
    "    baseline_tbl.to_csv(OUT / \"baseline_metrics_summary.csv\", index=False)\n",
    "if 'baseline_rank_tbl' in globals() and not baseline_rank_tbl.empty:\n",
    "    baseline_rank_tbl.to_csv(OUT / \"baseline_winner_rank_summary.csv\", index=False)\n",
    "\n",
    "print(\"[OK] exported to:\", OUT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-awards",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
